# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NI6pW3ki5wiWOSNh1d1NtJ0kbIY5VdSk
"""

# imports for EDA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Import GridSpec instead of Gridspec
from matplotlib.gridspec import GridSpec
from scipy.stats import chi2_contingency,f_oneway,jarque_bera,probplot
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import data_clean_utils
import missingno as msno

# load the data
df = pd.read_csv('swiggy.csv')
df.head()

data_clean_utils.perform_data_cleaning(data=df)

df_final = pd.read_csv('swiggy_cleaned.csv')
df_final.head()

"""# Preliminary Analysis #"""

df_final.isnull().sum()

missing_rowa = (
    df_final.isnull()
    .any(axis=1)
    .sum()
)

print(f'There are {missing_rowa} rows with missing values')
print(f"it accounts for {(missing_rowa/df_final.shape[0])*100:.2f}% of the data")

df_final.duplicated().sum()

df_final.dtypes

df_final.describe()

# summary for categorical columns
df_final.describe(include='object')



"""# matrix plot"""

msno.matrix(df_final)

msno.heatmap(df_final)

msno.dendrogram(df_final)



"""# Function to perform analysis"""

def numerical_analysis(dataframe,column_name,cat_col=None,bins="auto"):
  #created the figure
  fig = plt.figure(figsize=(15,10))
  #gernated the layout
  grid = GridSpec(ncols=2,nrows=2,figure=fig)
  #set subplot
  ax1 = fig.add_subplot(grid[0,0])
  ax2 = fig.add_subplot(grid[0,1])
  ax3 = fig.add_subplot(grid[1,:])
  #plot the kde plot
  sns.kdeplot(data=dataframe,x=column_name,hue=cat_col,ax=ax1)
  #plot the box plot
  sns.boxplot(data=dataframe,x=column_name,hue=cat_col,ax=ax2)
  #plot the histogram
  sns.histplot(data=dataframe,x=column_name,ax=ax3,bins=bins,hue=cat_col,kde=True)

  plt.tight_layout()
  plt.show()

def numerical_categorical_analysis(dataframe,cat_column_1,num_column):
  fig,(ax1,ax2) = plt.subplots(2,2, figsize=(15,7.5))
  # plot the barplot
  sns.barplot(data=dataframe,x=cat_column_1,y=num_column,ax=ax1[0])
  #plot the box
  sns.boxplot(data=dataframe, x=cat_column_1,y=num_column,ax = ax1[1])
  #plot violin plot
  sns.violinplot(data=dataframe,x=cat_column_1,y=num_column,ax = ax2[0])
  #stripplot helps to see data distribution
  sns.stripplot(data=dataframe,x=cat_column_1,y=num_column,ax = ax2[1])
  plt.tight_layout()
  plt.show()

def categorical_analysis(dataframe,column_name):
  display(
      pd.DataFrame({
          "count":(
              dataframe[column_name]
              .value_counts()),
          "percentage":(
                dataframe[column_name]
              .value_counts(normalize=True)
              .mul(100)
              .round(2)
              .astype("str")
              .add("%")
            )
      })
  )
  print("*" * 50)
  #get unique categories
  unique_categories = dataframe[column_name].unique().tolist()
  number_of_categories = dataframe[column_name].nunique()
  print(f"The unique categories in {column_name} are {unique_categories}")
  print("*" * 50)
  print(f"The number of categories in {column_name} is {number_of_categories}")

  sns.countplot(data=dataframe,x=column_name)
  plt.xticks(rotation=45)
  plt.show()

def multivarient_analysis(dataframe,num_column,cat_column_1,cat_column_2):
  fig,(ax1,ax2) = plt.subplots(2,2, figsize=(15,7.5))
  #plot the barplot
  sns.barplot(data=dataframe,x=cat_column_1,y=num_column,hue=cat_column_2,ax=ax1[0])
  #plot the box plot
  sns.boxplot(data=dataframe,x=cat_column_1,y=num_column,hue=cat_column_2,gap=0.1,ax=ax1[1])
  #plot violin plot
  sns.violinplot(data=dataframe,x=cat_column_1,y=num_column,hue=cat_column_2,ax=ax2[0],gap=0.1)
  #stripplot helps to see data distribution
  sns.stripplot(data=dataframe,x=cat_column_1,y=num_column,hue=cat_column_2,ax=ax2[1],jitter=True,dodge=True)
  plt.tight_layout()
  plt.show()

import pandas as pd
from scipy.stats import chi2_contingency

def chi_2_test(dataframe, col1, col2, alpha=0.05):
    # Subset the dataframe to include only the columns of interest and drop rows with NaN values
    data = dataframe.loc[:, [col1, col2]].dropna()

    # Create the contingency table
    contingency_table = pd.crosstab(data[col1], data[col2])

    # Perform the Chi-Square test
    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
    # Hypothesis testing
    if p_val <= alpha:
        print("Reject the null hypothesis (There is a significant association between the variables)")
    else:
        print("Accept the null hypothesis (There is no significant association between the variables)")

def anova_test(dataframe,num_col1,cat_col2, alpha=0.05):
  data = (
      dataframe.loc[:,[num_col1,cat_col2]]
      .dropna()
  )
  cat_group = data.groupby(cat_col2)
  groups = [group[num_col1].values for _,group in cat_group]
  f_stat,p_val = f_oneway(*groups)
  print(f_stat,p_val)
  if p_val <= alpha:
    print("Reject the null hypothesis")
  else:
    print("Accept the null hypothesis")

def test_for_normality(dataframe,column_name,alpha=0.05):
  data = dataframe[column_name]
  print("jarque bera Test for Normality")
  _,p_val = jarque_bera(data)
  print(p_val)
  if p_val <= alpha:
    print("Reject the null hypothesis")
  else:
    print("Accept the null hypothesis and the data is normaliy distributed")



"""#column Wise analysis

"""

df_final.columns

df_final['time_taken'].dtype

#numberical col anlysis
numerical_analysis(df_final,'time_taken',bins=10)



"""Observation


1.   The target column is not fully continouse
2.   the target column shows dual modality with two peaks-one peak around the 17-18 mark and other around 26-27 mar




"""

probplot(df_final['time_taken'],plot=plt)
plt.show()

#tests for normality
test_for_normality(df_final,'time_taken')

#check out the rows where data is acting as outlier
target_25_per, target_75_per = np.percentile(df_final['time_taken'],[25,75])
iqr = target_75_per - target_25_per

upper_bound = target_75_per + (1.5* iqr)

df_final.loc[(df_final['time_taken'] > upper_bound)] ['traffic'].value_counts()

df_final.loc[(df_final['time_taken'] > upper_bound)] ['weather'].value_counts()

#to fix the target column using transform
from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method='yeo-johnson')
df_final['time_taken'] = pt.fit_transform(df_final[['time_taken']])

numerical_analysis(df_final,'time_taken',bins=10)



"""# Rider id"""

df_final[['rider_id','age','ratings']]



"""# age"""

df_final['age'].dtype

df_final['age'].describe()

#numerical analyis
numerical_analysis(df_final,'age',bins=10)

sns.scatterplot(data=df_final,x='age',y='time_taken')
plt.show()



"""age of the rider dose not impact on time  taken to deliver"""

sns.scatterplot(data=df_final,x='age',y='time_taken',hue='vehicle_condition')
plt.show()



"""# ratings"""

df_final['ratings'].dtype

df_final['ratings'].describe()

#numerical analyiss
numerical_analysis(df_final,'ratings',bins=10)

# analyis between rating and vehcial condition
numerical_categorical_analysis(df_final,'vehicle_condition','ratings')

#analysis between rating and festivel
numerical_categorical_analysis(df_final,'festival','ratings')

"""# Location based feature

"""

df_final.columns[3:7].tolist()+["city_name"]

#location subset
location_subset = df_final.loc[:,df_final.columns[3:7].tolist()+["city_name"]]

location_subset

#drop missing values
location_subset.dropna(inplace=True)

# plot deliveries on map
delivery_df = pd.DataFrame({
    'latitude': location_subset['delivery_latitude'],
    'longitude': location_subset['delivery_longitude'],
    'city_name': location_subset['city_name']
})
#Create a map using Plotly's scatter_mapbox
fig = px.scatter_mapbox(
    delivery_df,
    lat='latitude',
    lon='longitude',
    title ='Delivery points',
    hover_name ='city_name'
)
#update the layout for the map of india
fig.update_layout(
    mapbox_style= "carto-positron",
    mapbox_center = {"lat":20.5937,"lon":78.9629},
    mapbox_zoom = 3
)
fig.show()



"""# Order date"""

df_final.columns

df_final.filter(like="order")

order_date_subset = df_final.loc[:,["order_date","order_day","order_month","order_day_of_week","is_weekend","festival"]]

order_date_subset

#  analysis between day_of_week and target
numerical_categorical_analysis(df_final,'order_day_of_week','time_taken')

# is_weekend and target
numerical_categorical_analysis(df_final,'is_weekend','time_taken')

chi_2_test(df_final,"is_weekend","traffic")

#festivel and target
numerical_categorical_analysis(df_final,'festival','time_taken')

#do festivel affect traffic
chi_2_test(df_final,"festival","traffic")

# dose a weekend and a festival combined an affect on delivery time
multivarient_analysis(df_final,'time_taken','is_weekend','festival')



"""# Order Time"""

df_final.columns

#time related col
time_related_cols = df_final.loc[:,["order_time_hour","order_time_of_day","pickup_time_minutes"]]

time_related_cols

#dose delivery time affected by time_oof day
numerical_categorical_analysis(df_final,'order_time_of_day','time_taken')

#anvo test
anova_test(df_final,'time_taken','order_time_of_day')

#categorical analysis on order_Time
categorical_analysis(df_final,"order_time_hour")

#categorical analyis on time of day
categorical_analysis(df_final,"order_time_of_day")



"""# traffic"""

df_final['traffic'].dtype

# categorical analyis on traffic
categorical_analysis(df_final,"traffic")

#dose traffic deepends on city_type
chi_2_test(df_final,"traffic","city_type")

#are some vehicle type more suitable in trafic then other
multivarient_analysis(df_final,'time_taken','traffic','type_of_vehicle')

multivarient_analysis(df_final,'time_taken','traffic','vehicle_condition')

multivarient_analysis(df_final,'time_taken','festival','vehicle_condition')

numerical_categorical_analysis(df_final,'multiple_deliveries','time_taken')

